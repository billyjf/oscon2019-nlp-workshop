{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A hands-on introduction to Natural Language Processing in Python\n",
    "#### 16th July, 2019 at OSCON\n",
    "## Pre-requisites\n",
    "Access the notebook at \n",
    "\n",
    "Please have the following pre-requisites ready on your machine. If you don't have them, please install them with the hurry of this hacker cat!\n",
    "\n",
    "![Typing cat gif](https://media.giphy.com/media/o0vwzuFwCGAFO/giphy.gif)\n",
    "\n",
    "### Installing Jupyter on your machine\n",
    "\n",
    "- If you already have Anaconda, you should have Jupyter installed as well. ```jupyter notebook``` opens up a Jupyter notebook in your browser at default port 8888. \n",
    "\n",
    "- Use pip install. ```python3 -m pip install jupyter``` for Python 3 install, ```python -m pip install jupyter``` for Python 2 install. For more details, refer to https://jupyter.org/install\n",
    "\n",
    "- Also use pip to install the following packages: ```nltk, lxml, requests, matplotlib, re, sklearn, wikipedia, random, gensim, wordcloud```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK\n",
    "Natural Language Toolkit is the most popular collection of libraries and programs to do NLP. You can find more about it here: http://www.nltk.org/book/ch00.html. We will mainly be using NLTK to perform different tasks, along with a few other packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/billyfisher/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /Users/billyfisher/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK comes pre-loaded with texts from the Project Gutenberg archive that you can use. It also has a collection of informal text from discussion forums, conversations, chat sessions, movie scripts, etc. NLTK has corpora in other languages as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "austen-emma.txt [Emma by Jane Austen 1816]\n",
      "\n",
      "VOLUME I\n",
      "\n",
      "CHAPTER I\n",
      "\n",
      "\n",
      "Emma Woodhouse, handsome, clever, and rich, with a ...\n",
      "austen-persuasion.txt [Persuasion by Jane Austen 1818]\n",
      "\n",
      "\n",
      "Chapter 1\n",
      "\n",
      "\n",
      "Sir Walter Elliot, of Kellynch Hall, in Somersetshire ...\n",
      "austen-sense.txt [Sense and Sensibility by Jane Austen 1811]\n",
      "\n",
      "CHAPTER 1\n",
      "\n",
      "\n",
      "The family of Dashwood had long been settle ...\n",
      "bible-kjv.txt [The King James Bible]\n",
      "\n",
      "The Old Testament of the King James Bible\n",
      "\n",
      "The First Book of Moses:  Called  ...\n",
      "blake-poems.txt [Poems by William Blake 1789]\n",
      "\n",
      " \n",
      "SONGS OF INNOCENCE AND OF EXPERIENCE\n",
      "and THE BOOK of THEL\n",
      "\n",
      "\n",
      " SONGS  ...\n",
      "bryant-stories.txt [Stories to Tell to Children by Sara Cone Bryant 1918] \r\n",
      "\r\n",
      "\r\n",
      "TWO LITTLE RIDDLES IN RHYME\r\n",
      "\r\n",
      "\r\n",
      "     T ...\n",
      "burgess-busterbrown.txt [The Adventures of Buster Bear by Thornton W. Burgess 1920]\r\n",
      "\r\n",
      "I\r\n",
      "\r\n",
      "BUSTER BEAR GOES FISHING\r\n",
      "\r\n",
      "\r\n",
      "Bu ...\n",
      "carroll-alice.txt [Alice's Adventures in Wonderland by Lewis Carroll 1865]\n",
      "\n",
      "CHAPTER I. Down the Rabbit-Hole\n",
      "\n",
      "Alice was ...\n",
      "chesterton-ball.txt [The Ball and The Cross by G.K. Chesterton 1909]\n",
      "\n",
      "\n",
      "I. A DISCUSSION SOMEWHAT IN THE AIR\n",
      "\n",
      "The flying s ...\n",
      "chesterton-brown.txt [The Wisdom of Father Brown by G. K. Chesterton 1914]\n",
      "\n",
      "\n",
      "I. The Absence of Mr Glass\n",
      "\n",
      "\n",
      "THE consulting- ...\n",
      "chesterton-thursday.txt [The Man Who Was Thursday by G. K. Chesterton 1908]\n",
      "\n",
      "To Edmund Clerihew Bentley\n",
      "\n",
      "A cloud was on the  ...\n",
      "edgeworth-parents.txt [The Parent's Assistant, by Maria Edgeworth]\r\n",
      "\r\n",
      "\r\n",
      "THE ORPHANS.\r\n",
      "\r\n",
      "Near the ruins of the castle of Ro ...\n",
      "melville-moby_dick.txt [Moby Dick by Herman Melville 1851]\r\n",
      "\r\n",
      "\r\n",
      "ETYMOLOGY.\r\n",
      "\r\n",
      "(Supplied by a Late Consumptive Usher to a Gr ...\n",
      "milton-paradise.txt [Paradise Lost by John Milton 1667] \n",
      " \n",
      " \n",
      "Book I \n",
      " \n",
      " \n",
      "Of Man's first disobedience, and the fruit \n",
      "Of  ...\n",
      "shakespeare-caesar.txt [The Tragedie of Julius Caesar by William Shakespeare 1599]\n",
      "\n",
      "\n",
      "Actus Primus. Scoena Prima.\n",
      "\n",
      "Enter Fla ...\n",
      "shakespeare-hamlet.txt [The Tragedie of Hamlet by William Shakespeare 1599]\n",
      "\n",
      "\n",
      "Actus Primus. Scoena Prima.\n",
      "\n",
      "Enter Barnardo a ...\n",
      "shakespeare-macbeth.txt [The Tragedie of Macbeth by William Shakespeare 1603]\n",
      "\n",
      "\n",
      "Actus Primus. Scoena Prima.\n",
      "\n",
      "Thunder and Lig ...\n",
      "whitman-leaves.txt [Leaves of Grass by Walt Whitman 1855]\n",
      "\n",
      "\n",
      "Come, said my soul,\n",
      "Such verses for my Body let us write, ( ...\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "\n",
    "# Printing the first 100 characters of each of the files\n",
    "for fileid in gutenberg.fileids():\n",
    "    print(fileid, gutenberg.raw(fileid)[:100], '...')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data\n",
    "![Fetch data](https://media.giphy.com/media/4FQMuOKR6zQRO/giphy.gif)\n",
    "\n",
    "Data can come from a variety of sources in different formats. Natural language can be in the form of text or speech. For the purpose of this tutorial, we will be focusing on text-based processing as opposed to speech recognition and synthesis. <br>\n",
    "\n",
    "Textual data can be stored in databases, dataframes, text files, webpages, etc. A list of text datasets can be found here: https://github.com/niderhoff/nlp-datasets. Let's create a list with a few sentences that will serve as the sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = [\"Today is 16th July. I am in Portland, Oregon for OSCON. Currently I am \" \\\n",
    "                \"attending a Natural Language Processing tutorial.\"]\n",
    "sample_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also scrape a webpage using the `requests` and `lxml` libraries. Let's trying scraping a paragraph from the landing page for OSCON. Use 'Inspect' functionality for the webpage to get the XPath for a particular element. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import html\n",
    "import requests\n",
    "\n",
    "# Scraping data from a webpage element\n",
    "page = requests.get('https://conferences.oreilly.com/oscon/oscon-or')\n",
    "tree = html.fromstring(page.content)\n",
    "conference_data = tree.xpath('//*[@id=\"reasons_row\"]/div/p[1]/text()') \n",
    "print(conference_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence segmentation\n",
    "A paragraph is nothing but a collection of sentences. Also called sentence tokenization or sentence boundary disambiguation, this process breaks up sentences by deciding where a sentence starts and ends. Challenges include recognizing ambiguous puncutation marks. For example, `.` can be used for a decimal point, an ellipsis or a period. Let's use ```sent_tokenize``` from ```nltk.tokenize``` to get sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def get_sent_tokens(data):\n",
    "    \"\"\"Sentence tokenization\"\"\"\n",
    "    sentences = []\n",
    "    for sent in data:\n",
    "        sentences.extend(sent_tokenize(sent))\n",
    "    print('Sentence tokens:', sentences)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sentences = get_sent_tokens(sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conference_sentences = get_sent_tokens(conference_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word tokenization\n",
    "A sentence is a collection of words. Word tokenization is similar to sentence tokenization, but works on words. Let's use ```word_tokenize``` from ```nltk.tokenize``` to get the words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def get_word_tokens(sentences):\n",
    "    '''Word tokenization'''\n",
    "    words = []\n",
    "    for sent in sentences:\n",
    "        words.extend(word_tokenize(sent))\n",
    "    print('Word tokens:', words)\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_words = get_word_tokens(sample_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conference_words = get_word_tokens(conference_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency distribution\n",
    "Calculates the frequency distribution for each word in the data. Use ```nltk.probability``` from ```FreqDist``` and ```matplotlib```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "from nltk.probability import FreqDist\n",
    "matplotlib.use('TkAgg') \n",
    "\n",
    "def plot_freq_dist(words, num_words = 20):\n",
    "    '''Frequency distribution'''\n",
    "    fdist = FreqDist(words)\n",
    "    fdist.plot(num_words, cumulative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plot_freq_dist(sample_words, num_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_freq_dist(conference_words, num_words=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the data\n",
    "![Cleaning data](https://media.giphy.com/media/10zsjaH4g0GgmY/giphy.gif)\n",
    "\n",
    "Oops, we missed a crucial step! Real world data is often messy and needs to undergo cleaning. You can do a bunch of preprocessing to ensure the data is clean, like:\n",
    "- Removing special characters and numbers - These are usually not important when trying to derive the semantics\n",
    "- Removing stopwords - A special category of words that don't have any significance on their own and are often used as filler words or to ensure correct grammer. Eg. the, and, but, of, is, or, those, her, \n",
    "- Removing HTML tags - Raw data from webpages can often be laden with HTML tags. Use a library like `BeautifulSoup` to process and remove the tags.\n",
    "- Standardizing words - This aims to consolidate different versions of the same version Eg. SMS/Twitter language, slang, misspellings \n",
    "- Converting to lower case - To ensure uniformity across all words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def get_clean_sentences(sentences, remove_digits=False):\n",
    "    '''Cleaning sentences by removing special characters and optionally digits'''\n",
    "    clean_sentences = []\n",
    "    for sent in sentences:\n",
    "        pattern = r'[^a-zA-Z0-9\\s]' if not remove_digits else r'[^a-zA-Z\\s]' \n",
    "        clean_text = re.sub(pattern, '', sent)\n",
    "        clean_text = clean_text.lower()\n",
    "        clean_sentences.append(clean_text)\n",
    "    print('Clean sentences:', clean_sentences)\n",
    "    return clean_sentences\n",
    "\n",
    "def filter_stopwords(words):\n",
    "    '''Removing stopwords from given words'''\n",
    "    filtered_words = [w for w in words if w not in stop_words]\n",
    "    print('Filtered words:', filtered_words)\n",
    "    return filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sentences = get_clean_sentences(sample_sentences, remove_digits = True)\n",
    "sample_words = get_word_tokens(sample_sentences)\n",
    "sample_words = filter_stopwords(sample_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conference_sentences = get_clean_sentences(conference_sentences)\n",
    "conference_words = get_word_tokens(conference_sentences)\n",
    "conference_words = filter_stopwords(conference_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After cleaning the text and using tokenization, we are left with words. Words have certain properties which we'll be exploring in the next few sections. These characteristics can often be used as features for a Machine Learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS tagging\n",
    "\n",
    "The English language is formed of different parts of speech (POS) like nouns, verbs, pronouns, adjectives, etc. POS tagging analyzes the words in a sentences and associates it with a POS tag depending on the way it is used. Also called grammatical tagging or word-category disambiguation. Use ```nltk.pos_tag``` for the process. There are different types of tagsets used with the most common being the Penn Treebank tagset and the Universal tagset. \n",
    "\n",
    "![Penn POS tags](https://slideplayer.com/slide/6855236/23/images/11/Penn+TreeBank+POS+Tag+set.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def get_pos_tags(words):\n",
    "    '''Get the part of speech (POS) tags for the words'''\n",
    "    tags=[]\n",
    "    for word in words:\n",
    "        tags.append(nltk.pos_tag([word]))\n",
    "    print(tags)\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_tags = get_pos_tags(sample_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conference_tags = get_pos_tags(conference_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text processing\n",
    "Text processing approaches like stemming and lemmatization help in reducing inflectional forms of words. \n",
    "### Dictionary and thesaurus\n",
    "WordNet is a lexical database that also has relationships between different words. You can use synsets to find definitions, synonyms and antonyms for words. You can also find hyponyms and hypernyms using the same process. Hypernym is a generalized concept like 'programming language' whereas hyponym is a specific concept like 'Python' or 'Java'.\n",
    "\n",
    "![Hypernym and hyponym](https://upload.wikimedia.org/wikipedia/en/thumb/1/1f/Hyponymsandhypernyms.jpg/300px-Hyponymsandhypernyms.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_wordnet_properties(words):\n",
    "    '''Returns definition, synonyms and antonyms of words'''\n",
    "    for word in words:\n",
    "        synonyms = []\n",
    "        antonyms = []\n",
    "#         hyponyms = []\n",
    "#         hypernyms = []\n",
    "        definitions = []\n",
    "        for syn in wordnet.synsets(word):\n",
    "            for lm in syn.lemmas():\n",
    "                synonyms.append(lm.name())\n",
    "                if lm.antonyms(): \n",
    "                    antonyms.append(lm.antonyms()[0].name())\n",
    "#             hyponyms.append(syn.hyponyms())\n",
    "#             hypernyms.append(syn.hypernyms())\n",
    "#             definitions.append(syn.definition())\n",
    "            \n",
    "        print(word)\n",
    "        print('Synonyms:', synonyms, '\\nAntonyms:', antonyms, '\\n')\n",
    "#         print('Definition:', definitions, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have you watched the series 'Friends'? Do you remember the [episode](https://youtu.be/B1tOqZUNebs?t=100) where Joey has to write a letter of recommendation for Monica and Chandler for the adoption agency? He uses a thesaurus to make himself sound smarter in the letter! Let's see if we get the same results:\n",
    "\n",
    "'They are warm, nice people with big hearts' -> 'They are humid, prepossessing Homo Sapiens with full-sized aortic pumps'\n",
    "\n",
    "![Joey Friends](https://media.giphy.com/media/VEsfbW0pBu145PPhOi/giphy.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "joey_dialogue = ['they', 'are', 'warm', 'nice', 'people', 'with', 'big', 'hearts']\n",
    "get_wordnet_properties(joey_dialogue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Sense Disambiguation\n",
    "\n",
    "These synsets are also used for disambiguation, particularly Word Sense Disambiguation using Lesk Algorithm. See: http://www.nltk.org/howto/wsd.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.wsd import lesk\n",
    "sent = ['I', 'went', 'to', 'the', 'bank', 'to', 'deposit', 'money', '.']\n",
    "print(lesk(sent, 'bank', 'n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = ['I', 'was', 'sitting', 'by', 'the', 'bank', '.']\n",
    "print(lesk(sent, 'bank', 'n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "Stemming tries to cut off at the ends of the words in the hope of deriving the base form. Stems aren't always real words. Use ```PorterStemmer``` from ```ntlk.stem```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def get_stems(words):\n",
    "    '''Reduce the words to their base word (stem) by cutting off the ends'''\n",
    "    ps = PorterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stems.append(ps.stem(word))\n",
    "    print(stems)\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_stems = get_stems(sample_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conference_stems = get_stems(conference_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "Lemmatization groups different inflected forms of a words so they can be mapped to the same base. Lemmas are real words. More complex than stemming, context of words is also analyzed. Uses WordNet which is a lexical English database. \n",
    "Use ```WordNetLemmatizer``` from ```nltk.stem``` and provide it the POS tag along with the word. NLTK’s POS tags are in a format different from to that of wordnet lemmatizer, so a mapping is needed. https://stackoverflow.com/questions/15586721/wordnet-lemmatization-and-pos-tagging-in-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_lemma(word_tags):\n",
    "    '''Reduce the words to their base word (lemma) by using a lexicon'''\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    lemma = []\n",
    "    for element in word_tags:\n",
    "        word = element[0][0]\n",
    "        pos = element[0][1]\n",
    "        tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "        tag_dict = {\"J\": wordnet.ADJ, # Mapping NLTK POS tags to WordNet POS tags\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "        wordnet_pos = tag_dict.get(tag, wordnet.NOUN)\n",
    "        lemma.append(wordnet_lemmatizer.lemmatize(word, wordnet_pos))\n",
    "    print(lemma)\n",
    "    return(lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_lemma = get_lemma(sample_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_lemma' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-402362f73db3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconference_lemma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_lemma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconference_tags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'get_lemma' is not defined"
     ]
    }
   ],
   "source": [
    "conference_lemma = get_lemma(conference_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These processes can create features that act as inputs to predictive models. It also helps in using lesser memory by making the data smaller and reducing the size of the vocabulary. Often times, these normalized words are sufficient to provide the semantics. Like in the case of understanding the meaning behind the sentences:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distances \n",
    "You can calculate distances between words. There are a variety of distance metrics available: https://en.wikipedia.org/wiki/String_metric. The most common ones are Levenshtein, Cosine distances and Jaccard similarity. Applications include spell checking, correction for OCRs and Machine Translation. For an implementation of a spell checker, see here: https://norvig.com/spell-correct.html\n",
    "\n",
    "![Edit distance](https://i.stack.imgur.com/5Pjr7.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition (NER) \n",
    "\n",
    "Also known as entity chunking or extraction, is a sub-process of information extraction. This involves identifies and classifies named entities mentions into sub-categories like person name, organization, location, time, etc.  In other words, Named Entity Recognition (NER) labels sequences of words in a text which are the names of things, such as person and company names, or gene and protein names. \n",
    "\n",
    "Some of the most popular NER models are here: https://towardsdatascience.com/a-review-of-named-entity-recognition-ner-using-automatic-summarization-of-resumes-5248a75de175. <br>\n",
    "\n",
    "Example use-cases include customer support, search engine, news classification. Another emerging application is for redacting personally identifiable information (PII). A great demo of NER in action is here: https://explosion.ai/demos/displacy-ent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words\n",
    "Bag of words is an approach for text feature extraction. Just imagine a bag of popcorn, \n",
    "and each popcorn kernel represents a word that is present in the text. Each sentence can be represented as a vector\n",
    "of all the words present in a vocabulary. If a word is present in the sentence, it is 1, otherwise 0.\n",
    "\n",
    "![Bag of words](https://cdn-images-1.medium.com/max/1600/1*zMdHVQQ7HYv_mMZ5Ne-2yQ.png)\n",
    "\n",
    "## TF-IDF\n",
    "Term-frequency inverse document frequency assigns scores to words inside a document. Commonly occuring words in all documents would have less weightage.\n",
    "![TF IDF](http://www.bloter.net/wp-content/uploads/2016/09/td-idf-graphic.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def get_bag_of_words(sentences):\n",
    "    ''''''\n",
    "    vectorizer = CountVectorizer()\n",
    "    print(vectorizer.fit_transform(sentences).todense())\n",
    "    print(vectorizer.vocabulary_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_bag_of_words(sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_bag_of_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-ecb24dc3c8ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_bag_of_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconference_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'get_bag_of_words' is not defined"
     ]
    }
   ],
   "source": [
    "get_bag_of_words(conference_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embeddings - Word2Vec\n",
    "Vector space model - represent words and sentences as vectors to get semantic relationships. A really good tutorial for Word2Vec is here: https://www.kaggle.com/alvations/word2vec-embedding-using-gensim-and-nltk\n",
    "\n",
    "![Word2Vec](http://www.flyml.net/wp-content/uploads/2016/11/w2v-3-samples.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning using Natural Language Processing\n",
    "Machine Learning includes two approaches: supervised and unsupervised. Supervised learning works on data that already has labels i.e. they provide supervision to the model. Eg. Classification, Regression. Unsupervised learning is to find out the inherent structure present in the data and there are no labels i.e. no supervision. Eg. Clustering.\n",
    "\n",
    "A lot of these text properties can be used as features for Machine Learning systems. One specific case is text classification. A more detailed resource is here: https://www.nltk.org/book/ch06.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis\n",
    "\n",
    "NLTK's VADER algorithm is used to detect polarity of words and establish the overall sentiment (compound score) for sentences. We're using a small sample of IMDB review titles for Captain Marvel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer \n",
    "\n",
    "def get_sentiment(data):\n",
    "    '''Get sentiment of sentences using VADER algorithm'''\n",
    "    scorer = SentimentIntensityAnalyzer()\n",
    "    for sentence in reviews:\n",
    "        print(sentence)\n",
    "        ss = scorer.polarity_scores(sentence)\n",
    "        for k in ss:\n",
    "            print('{0}: {1}, ' .format(k, ss[k]), end='')\n",
    "        print()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = [\"I've watched this movie for the record, cause I had no particular expectations for it.\",\n",
    "          \"It's fine.\",\n",
    "          \"Not bad, not great either.\",\n",
    "          \"Captain Marvel is another fine Marvel adventure.\",\n",
    "          \"Captivating story and great visuals.\",\n",
    "          \"Confusing boring childish.\",\n",
    "          \"Strong Hero. Weak Film.\"]\n",
    "\n",
    "get_sentiment(reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic modeling\n",
    "Topic modeling is an unsupervised ML method used to find inherent structure in documents. It learns\n",
    "representations of topics in documents which allows grouping of different documents together. We will\n",
    "use ```Gensim``` library and Latent Dirichlet Allocation (LDA) for this.\n",
    "\n",
    "LDA’s approach to topic modeling is it considers each document as a collection of topics in a certain proportion. And each topic as a collection of keywords, again, in a certain proportion.\n",
    "\n",
    "Once you provide the algorithm with the number of topics, all it does it to rearrange the topics distribution within the documents and keywords distribution within the topics to obtain a good composition of topic-keywords distribution.\n",
    "\n",
    "Adapted from https://kleiber.me/blog/2017/07/22/tutorial-lda-wikipedia/\n",
    "\n",
    "![Topic modeling](https://i.stack.imgur.com/vI8Lc.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia, random\n",
    "\n",
    "def fetch_data(article_names):\n",
    "    '''Fetching the data from given Wikipedia articles'''\n",
    "    wikipedia_random_articles = wikipedia.random(2)\n",
    "    wikipedia_random_articles.extend(article_names)\n",
    "    wikipedia_random_articles\n",
    "    print(wikipedia_random_articles)\n",
    "    \n",
    "    wikipedia_articles = []\n",
    "    for wikipedia_article in wikipedia_random_articles:\n",
    "        wikipedia_articles.append([wikipedia_article, \n",
    "                                   wikipedia.page(wikipedia_article).content])\n",
    "    return wikipedia_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')    \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "def clean(article):\n",
    "    '''Cleaning the article contents and getting the word stems'''\n",
    "    title, document = article\n",
    "    tokens = RegexpTokenizer(r'\\w+').tokenize(document.lower())\n",
    "    tokens_clean = [token for token in tokens if token not in \n",
    "                    stopwords.words('english')]\n",
    "    tokens_stemmed = [PorterStemmer().stem(token) for token \n",
    "                      in tokens_clean]\n",
    "    return (title, tokens_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "import gensim\n",
    "\n",
    "article_names = ['Music', 'Grace Hopper', 'Omaha', 'Data', 'Compiler']\n",
    "wikipedia_articles = fetch_data(article_names)\n",
    "wikipedia_articles\n",
    "wikipedia_articles_clean = list(map(clean, wikipedia_articles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_contents = [article[1] for article in wikipedia_articles_clean]\n",
    "dictionary = corpora.Dictionary(article_contents)\n",
    "corpus = [dictionary.doc2bow(article) for article in \n",
    "          article_contents[:-1]] # All except 'Compiler'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus, num_topics=6, \n",
    "                                            id2word = dictionary, \n",
    "                                            passes=100)\n",
    "\n",
    "topic_results = lda_model.print_topics(num_topics=6, num_words=5)\n",
    "topic_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n",
    "\n",
    "cloud = WordCloud(background_color='white',\n",
    "                  width=2500,\n",
    "                  height=1800,\n",
    "                  max_words=10,\n",
    "                  colormap='tab10',\n",
    "                  color_func=lambda *args, **kwargs: cols[i],\n",
    "                  prefer_horizontal=1.0)\n",
    "\n",
    "topics = lda_model.show_topics(formatted=False)\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(10,10), sharex=True, sharey=True)\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    fig.add_subplot(ax)\n",
    "    topic_words = dict(topics[i][1])\n",
    "    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
    "    plt.gca().imshow(cloud)\n",
    "    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
    "    plt.gca().axis('off')\n",
    "\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.axis('off')\n",
    "plt.margins(x=0, y=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(lda_model[[dictionary.doc2bow(article_contents[-1])]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "- More on NLP https://monkeylearn.com/blog/definitive-guide-natural-language-processing/\n",
    "- A very comprehensive list of resources by Penn https://www.seas.upenn.edu/~romap/nlp-resources.html\n",
    "- Peter Norvig's spell corrector http://norvig.com/spell-correct.html\n",
    "- Applications and datasets https://machinelearningmastery.com/datasets-natural-language-processing/\n",
    "- More datasets https://gengo.ai/datasets/the-best-25-datasets-for-natural-language-processing/\n",
    "- https://towardsdatascience.com/text-analytics-topic-modelling-on-music-genres-song-lyrics-deb82c86caa2\n",
    "- Collection of tutorials https://medium.com/machine-learning-in-practice/over-200-of-the-best-machine-learning-nlp-and-python-tutorials-2018-edition-dd8cf53cb7dc\n",
    "- Text classification https://textminingonline.com/dive-into-nltk-part-vii-a-preliminary-study-on-text-classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contact\n",
    "\n",
    "You can contact me at:\n",
    "\n",
    "- Personal website: https://gjena.github.io/\n",
    "- LinkedIn: https://www.linkedin.com/in/grishmajena/\n",
    "- Twitter: @DebateLover"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
